"""
ZED VSLAM Node with RTABMap Occupancy Grid

This node implements a Visual SLAM system using the ZED 2 SDK with:
- Positional tracking with area memory for loop closure
- Occupancy grid mapping via rtabmap_ros
- Pose and odometry publishing
- Support for SVO file playback

RTABMap Integration:
This node publishes RGB images, depth images, camera info, and odometry to topics
that rtabmap_ros subscribes to. The occupancy grid is generated by rtabmap_ros and
this node subscribes to it, then republishes it for backward compatibility.

Required rtabmap_ros topics (configurable via parameters):
- Input: /zed_vslam/rgb/image_raw, /zed_vslam/depth/image_raw, /zed_vslam/rgb/camera_info, /zed_vslam/depth/camera_info, /zed_vslam/odom
- Output: /rtabmap/grid_map (subscribed by this node)

To use this node:
1. Launch rtabmap_ros (e.g., ros2 launch rtabmap_launch rtabmap.launch.py) 
2. Launch this node
3. The occupancy grid will be available on /zed_vslam/occupancy_grid

For more information, see: https://github.com/introlab/rtabmap_ros
"""

import numpy as np
import cv2

import rclpy
from rclpy.node import Node
from rclpy.qos import QoSProfile, ReliabilityPolicy, DurabilityPolicy, qos_profile_sensor_data
from sensor_msgs.msg import Image, PointCloud2, CameraInfo
from nav_msgs.msg import OccupancyGrid, Odometry
from geometry_msgs.msg import PoseStamped, TransformStamped
from cv_bridge import CvBridge
from tf2_ros import TransformBroadcaster

# Try to import ZED SDK
try:
    import pyzed.sl as sl
    ZED_SDK_AVAILABLE = True
except ImportError:
    ZED_SDK_AVAILABLE = False
    print("Warning: ZED SDK not available. Please install pyzed.")


class ZEDVSLAMNode(Node):
    """
    ROS2 Node for VSLAM with occupancy grid mapping using rtabmap_ros
    """
    
    def __init__(self, svo_filename=None, area_file_path=None, 
                 publish_rate=10.0):
        super().__init__('zed_vslam_node')
        
        if not ZED_SDK_AVAILABLE:
            self.get_logger().error("ZED SDK not available. Cannot initialize VSLAM.")
            raise RuntimeError("ZED SDK required but not available")
        
        self.bridge = CvBridge()
        self.svo_filename = svo_filename
        self.area_file_path = area_file_path
        
        # Declare ROS2 parameters for rtabmap_ros topic names (configurable)
        # Using zed_vslam namespace for ZED SDK direct integration
        self.declare_parameter('rgb_topic', '/zed_vslam/rgb/image_raw')
        self.declare_parameter('depth_topic', '/zed_vslam/depth/image_raw')
        self.declare_parameter('rgb_camera_info_topic', '/zed_vslam/rgb/camera_info')
        self.declare_parameter('depth_camera_info_topic', '/zed_vslam/depth/camera_info')
        self.declare_parameter('odom_topic', '/zed_vslam/odom')
        self.declare_parameter('rtabmap_grid_map_topic', '/rtabmap/grid_map')
        self.declare_parameter('occupancy_grid_topic', '/zed_vslam/occupancy_grid')
        self.declare_parameter('camera_frame_id', 'zed_camera_center')
        
        # Get topic names from parameters
        rgb_topic = self.get_parameter('rgb_topic').get_parameter_value().string_value
        depth_topic = self.get_parameter('depth_topic').get_parameter_value().string_value
        rgb_camera_info_topic = self.get_parameter('rgb_camera_info_topic').get_parameter_value().string_value
        depth_camera_info_topic = self.get_parameter('depth_camera_info_topic').get_parameter_value().string_value
        odom_topic = self.get_parameter('odom_topic').get_parameter_value().string_value
        rtabmap_grid_map_topic = self.get_parameter('rtabmap_grid_map_topic').get_parameter_value().string_value
        occupancy_grid_topic = self.get_parameter('occupancy_grid_topic').get_parameter_value().string_value
        self.camera_frame_id = self.get_parameter('camera_frame_id').get_parameter_value().string_value
        
        # Initialize ZED camera
        self.zed = None
        self.init_zed_camera()
        
        # Initialize positional tracking
        self.tracking_enabled = False
        self.enable_positional_tracking()
        
        # Camera pose and tracking state
        self.camera_pose = sl.Pose()
        self.tracking_state = sl.POSITIONAL_TRACKING_STATE.OFF
        self.initial_pose_set = False
        
        # TF broadcaster
        self.tf_broadcaster = TransformBroadcaster(self)
        
        # ROS2 Publishers for rtabmap_ros
        # RGB image publisher
        self.rgb_image_pub = self.create_publisher(
            Image,
            rgb_topic,
            qos_profile_sensor_data
        )
        
        # Depth image publisher
        self.depth_image_pub = self.create_publisher(
            Image,
            depth_topic,
            qos_profile_sensor_data
        )
        
        # Camera info publishers
        self.rgb_camera_info_pub = self.create_publisher(
            CameraInfo,
            rgb_camera_info_topic,
            qos_profile_sensor_data
        )
        
        self.depth_camera_info_pub = self.create_publisher(
            CameraInfo,
            depth_camera_info_topic,
            qos_profile_sensor_data
        )
        
        # Odometry publisher (for rtabmap)
        self.odom_pub = self.create_publisher(
            Odometry,
            odom_topic,
            10
        )
        
        # Additional publishers for compatibility
        self.pose_pub = self.create_publisher(
            PoseStamped,
            '/zed_vslam/pose',
            10
        )
        
        self.point_cloud_pub = self.create_publisher(
            PointCloud2,
            '/zed_vslam/point_cloud',
            10
        )
        
        # Subscribe to rtabmap occupancy grid
        qos_profile = QoSProfile(
            reliability=ReliabilityPolicy.BEST_EFFORT,
            durability=DurabilityPolicy.TRANSIENT_LOCAL,
            depth=10
        )
        
        self.occupancy_grid_sub = self.create_subscription(
            OccupancyGrid,
            rtabmap_grid_map_topic,
            self.occupancy_grid_callback,
            qos_profile
        )
        
        # Republish occupancy grid to original topic for compatibility
        self.occupancy_grid_pub = self.create_publisher(
            OccupancyGrid,
            occupancy_grid_topic,
            qos_profile
        )
        
        # Timer for processing frames
        self.timer_period = 1.0 / publish_rate
        self.timer = self.create_timer(self.timer_period, self.process_frame)
        
        # Frame counter for statistics
        self.frame_count = 0
        self.last_time = self.get_clock().now()
        
        # Store camera info for publishing
        self.camera_info_left = None
        self.stereo_transform = None
        self.baseline = 0.0
        
        self.get_logger().info('ZED VSLAM node initialized with rtabmap_ros integration')
        self.get_logger().info(f'Publishing RGB image to: {rgb_topic}')
        self.get_logger().info(f'Publishing depth image to: {depth_topic}')
        self.get_logger().info(f'Publishing RGB camera info to: {rgb_camera_info_topic}')
        self.get_logger().info(f'Publishing depth camera info to: {depth_camera_info_topic}')
        self.get_logger().info(f'Publishing odometry to: {odom_topic}')
        self.get_logger().info(f'Subscribing to rtabmap grid map: {rtabmap_grid_map_topic}')
        self.get_logger().info(f'Republishing occupancy grid to: {occupancy_grid_topic}')
        self.get_logger().info('Note: Ensure rtabmap_ros is running to generate the occupancy grid')
    
    def init_zed_camera(self):
        """Initialize ZED camera using ZED SDK"""
        try:
            self.zed = sl.Camera()
            init_params = sl.InitParameters()
            
            # Set resolution and depth mode
            init_params.camera_resolution = sl.RESOLUTION.HD720
            init_params.camera_fps = 30
            init_params.depth_mode = sl.DEPTH_MODE.NEURAL  # Use neural depth for better quality
            init_params.coordinate_units = sl.UNIT.METER
            init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP
            init_params.sdk_verbose = False
            
            # Handle SVO file input
            if self.svo_filename:
                init_params.set_from_svo_file(self.svo_filename)
                init_params.svo_real_time_mode = True
                self.get_logger().info(f"Using SVO file: {self.svo_filename}")
            
            # Open camera
            status = self.zed.open(init_params)
            if status != sl.ERROR_CODE.SUCCESS:
                self.get_logger().error(f"Failed to open ZED camera: {status}")
                raise RuntimeError(f"ZED camera initialization failed: {status}")
            
            # Get camera information
            cam_info = self.zed.get_camera_information()
            self.get_logger().info(f"ZED Camera opened successfully")
            self.get_logger().info(f"Resolution: {cam_info.camera_configuration.resolution.width}x{cam_info.camera_configuration.resolution.height}")
            self.get_logger().info(f"FPS: {cam_info.camera_configuration.fps}")
            
        except Exception as e:
            self.get_logger().error(f"Error initializing ZED camera: {e}")
            raise
    
    def enable_positional_tracking(self):
        """Enable positional tracking with area memory"""
        try:
            tracking_params = sl.PositionalTrackingParameters()
            
            # Enable area memory for loop closure and relocalization
            tracking_params.enable_area_memory = True
            tracking_params.enable_pose_smoothing = True
            tracking_params.enable_imu_fusion = True
            tracking_params.set_floor_as_origin = False
            tracking_params.set_gravity_as_origin = True
            
            # Load area file if provided
            if self.area_file_path:
                tracking_params.area_file_path = self.area_file_path
                self.get_logger().info(f"Loading area memory from: {self.area_file_path}")
            
            # Enable tracking
            status = self.zed.enable_positional_tracking(tracking_params)
            if status != sl.ERROR_CODE.SUCCESS:
                self.get_logger().error(f"Failed to enable positional tracking: {status}")
                return
            
            self.tracking_enabled = True
            self.get_logger().info("Positional tracking enabled with area memory")
            
        except Exception as e:
            self.get_logger().error(f"Error enabling positional tracking: {e}")
    
    def process_frame(self):
        """Process a frame from ZED camera"""
        if self.zed is None:
            return
        
        try:
            # Grab frame
            grab_status = self.zed.grab()
            if grab_status != sl.ERROR_CODE.SUCCESS:
                if self.frame_count % 100 == 0:  # Log occasionally to avoid spam
                    self.get_logger().warn(f'Failed to grab frame: {grab_status}')
                return
            
            # Get camera pose
            self.tracking_state = self.zed.get_position(
                self.camera_pose, 
                sl.REFERENCE_FRAME.WORLD
            )
            
            # Retrieve images and depth
            image = sl.Mat()   # Calls C constructor (an empty z matrix container)
            depth = sl.Mat()
            point_cloud = sl.Mat()
            
            retrieve_status_img = self.zed.retrieve_image(image, sl.VIEW.LEFT)
            retrieve_status_depth = self.zed.retrieve_measure(depth, sl.MEASURE.DEPTH)
            self.zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA)
            
            # Check if retrieval was successful
            if retrieve_status_img != sl.ERROR_CODE.SUCCESS:
                if self.frame_count % 100 == 0:
                    self.get_logger().warn(f'Failed to retrieve image: {retrieve_status_img}')
                return
            
            if retrieve_status_depth != sl.ERROR_CODE.SUCCESS:
                if self.frame_count % 100 == 0:
                    self.get_logger().warn(f'Failed to retrieve depth: {retrieve_status_depth}')
                return
            
            # Publish RGB and depth images for rtabmap_ros (always publish, even if tracking not OK)
            # Images are needed for rtabmap to work, tracking state only affects pose/odometry
            self.publish_rgb_image(image)
            self.publish_depth_image(depth)
            self.publish_camera_info()
            
            # Only publish pose/odometry when tracking is OK
            if self.tracking_state == sl.POSITIONAL_TRACKING_STATE.OK:
                self.publish_pose()
                self.publish_odometry()
                self.publish_tf()
            
            # Publish point cloud (optional, can be disabled for performance)
            # self.publish_point_cloud(point_cloud)
            
            self.frame_count += 1
            
            # Log statistics periodically
            if self.frame_count % 100 == 0:
                current_time = self.get_clock().now()
                elapsed = (current_time - self.last_time).nanoseconds / 1e9
                fps = 100.0 / elapsed if elapsed > 0 else 0
                self.get_logger().info(
                    f"Tracking state: {self.tracking_state}, "
                    f"FPS: {fps:.1f}, "
                    f"Frame: {self.frame_count}, "
                    f"Publishing RGB/depth images"
                )
                self.last_time = current_time
            
            # Log on first frame to confirm publishing started
            if self.frame_count == 1:
                self.get_logger().info('Started publishing RGB/depth images and camera info')
        
        except Exception as e:
            # Don't crash the node, just log and continue
            import traceback
            self.get_logger().error(f'Error in process_frame: {e}')
            self.get_logger().error(f'Traceback: {traceback.format_exc()}')
    
    def occupancy_grid_callback(self, msg):
        """
        Callback for rtabmap occupancy grid - republish to original topic
        """
        # Republish to maintain compatibility with existing subscribers
        self.occupancy_grid_pub.publish(msg)
    
    def publish_rgb_image(self, image):
        """Publish RGB image for rtabmap_ros"""
        try:
            # Get image data as numpy array
            image_data = image.get_data()
            
            if image_data is None or image_data.size == 0:
                if self.frame_count % 100 == 0:
                    self.get_logger().warn('RGB image data is None or empty')
                return
            
            # Handle different image formats from ZED SDK
            if len(image_data.shape) == 3:
                if image_data.shape[2] == 4:  # BGRA format
                    cv_image = cv2.cvtColor(image_data, cv2.COLOR_BGRA2BGR)
                elif image_data.shape[2] == 3:  # BGR format
                    cv_image = image_data
                else:
                    if self.frame_count % 100 == 0:
                        self.get_logger().warn(f'Unexpected image format: {image_data.shape}')
                    return
            else:
                if self.frame_count % 100 == 0:
                    self.get_logger().warn(f'Unexpected image dimensions: {image_data.shape}')
                return
            
            # Convert to ROS Image message
            ros_image = self.bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')
            ros_image.header.stamp = self.get_clock().now().to_msg()
            ros_image.header.frame_id = self.camera_frame_id
            
            self.rgb_image_pub.publish(ros_image)
        except Exception as e:
            if self.frame_count % 100 == 0:  # Log occasionally to avoid spam
                self.get_logger().warn(f'Failed to publish RGB image: {e}')
    
    def publish_depth_image(self, depth):
        """Publish depth image for rtabmap_ros"""
        try:
            # Get depth data as numpy array
            depth_data = depth.get_data()
            
            if depth_data is None or depth_data.size == 0:
                if self.frame_count % 100 == 0:
                    self.get_logger().warn('Depth image data is None or empty')
                return
            
            # Ensure depth data is float32 and 2D
            if not isinstance(depth_data, np.ndarray):
                depth_data = np.array(depth_data, dtype=np.float32)
            else:
                depth_data = depth_data.astype(np.float32)
            
            # Ensure it's 2D (height x width)
            if len(depth_data.shape) > 2:
                depth_data = depth_data[:, :, 0] if depth_data.shape[2] == 1 else depth_data
            
            # Convert to ROS Image message (32FC1 format for depth)
            ros_depth = self.bridge.cv2_to_imgmsg(depth_data, encoding='32FC1')
            ros_depth.header.stamp = self.get_clock().now().to_msg()
            ros_depth.header.frame_id = self.camera_frame_id
            
            self.depth_image_pub.publish(ros_depth)
        except Exception as e:
            if self.frame_count % 100 == 0:  # Log occasionally to avoid spam
                self.get_logger().warn(f'Failed to publish depth image: {e}')
    
    def publish_pose(self):
        """Publish camera pose"""
        if self.tracking_state != sl.POSITIONAL_TRACKING_STATE.OK:
            return
        
        pose_msg = PoseStamped()
        pose_msg.header.stamp = self.get_clock().now().to_msg()
        pose_msg.header.frame_id = 'map'
        
        translation = self.camera_pose.get_translation()
        orientation = self.camera_pose.get_orientation()
        
        pose_msg.pose.position.x = float(translation.get()[0])
        pose_msg.pose.position.y = float(translation.get()[1])
        pose_msg.pose.position.z = float(translation.get()[2])
        
        pose_msg.pose.orientation.x = float(orientation.get()[0])
        pose_msg.pose.orientation.y = float(orientation.get()[1])
        pose_msg.pose.orientation.z = float(orientation.get()[2])
        pose_msg.pose.orientation.w = float(orientation.get()[3])
        
        self.pose_pub.publish(pose_msg)
    
    def publish_odometry(self):
        """Publish odometry message for rtabmap_ros"""
        if self.tracking_state != sl.POSITIONAL_TRACKING_STATE.OK:
            return
        
        odom_msg = Odometry()
        odom_msg.header.stamp = self.get_clock().now().to_msg()
        odom_msg.header.frame_id = 'odom'  # rtabmap expects 'odom' frame
        odom_msg.child_frame_id = self.camera_frame_id
        
        translation = self.camera_pose.get_translation()
        orientation = self.camera_pose.get_orientation()
        
        odom_msg.pose.pose.position.x = float(translation.get()[0])
        odom_msg.pose.pose.position.y = float(translation.get()[1])
        odom_msg.pose.pose.position.z = float(translation.get()[2])
        
        odom_msg.pose.pose.orientation.x = float(orientation.get()[0])
        odom_msg.pose.pose.orientation.y = float(orientation.get()[1])
        odom_msg.pose.pose.orientation.z = float(orientation.get()[2])
        odom_msg.pose.pose.orientation.w = float(orientation.get()[3])
        
        # Set covariance (placeholder values)
        odom_msg.pose.covariance[0] = 0.01  # x
        odom_msg.pose.covariance[7] = 0.01  # y
        odom_msg.pose.covariance[14] = 0.01  # z
        odom_msg.pose.covariance[21] = 0.01  # roll
        odom_msg.pose.covariance[28] = 0.01  # pitch
        odom_msg.pose.covariance[35] = 0.01  # yaw
        
        self.odom_pub.publish(odom_msg)
    
    def publish_tf(self):
        """Publish transform from map to camera"""
        if self.tracking_state != sl.POSITIONAL_TRACKING_STATE.OK:
            return
        
        t = TransformStamped()
        t.header.stamp = self.get_clock().now().to_msg()
        t.header.frame_id = 'map'
        t.child_frame_id = self.camera_frame_id
        
        translation = self.camera_pose.get_translation()
        orientation = self.camera_pose.get_orientation()
        
        t.transform.translation.x = float(translation.get()[0])
        t.transform.translation.y = float(translation.get()[1])
        t.transform.translation.z = float(translation.get()[2])
        
        t.transform.rotation.x = float(orientation.get()[0])
        t.transform.rotation.y = float(orientation.get()[1])
        t.transform.rotation.z = float(orientation.get()[2])
        t.transform.rotation.w = float(orientation.get()[3])
        
        self.tf_broadcaster.sendTransform(t)
    
    def publish_camera_info(self):
        """Publish camera info for RGB and depth (rtabmap_ros expects both)"""
        if self.camera_info_left is None:
            cam_info = self.zed.get_camera_information()
            self.camera_info_left = cam_info.camera_configuration.calibration_parameters.left_cam
            # Get stereo baseline for projection matrix
            self.stereo_transform = cam_info.camera_configuration.calibration_parameters.stereo_transform.get_translation().get()
            self.baseline = abs(self.stereo_transform[1])  # Baseline in meters (Ty, y-component is negative)
            
            # Log camera model parameters for verification
            self.get_logger().info('Camera model parameters:')
            self.get_logger().info(f'  Resolution: {self.camera_info_left.image_size.width}x{self.camera_info_left.image_size.height}')
            self.get_logger().info(f'  Focal length: fx={self.camera_info_left.fx:.2f}, fy={self.camera_info_left.fy:.2f}')
            self.get_logger().info(f'  Principal point: cx={self.camera_info_left.cx:.2f}, cy={self.camera_info_left.cy:.2f}')
            self.get_logger().info(f'  Stereo baseline: {self.baseline:.4f} m')
            if len(self.camera_info_left.disto) >= 5:
                self.get_logger().info(f'  Distortion: k1={self.camera_info_left.disto[0]:.6f}, k2={self.camera_info_left.disto[1]:.6f}, '
                                     f'p1={self.camera_info_left.disto[2]:.6f}, p2={self.camera_info_left.disto[3]:.6f}, '
                                     f'k3={self.camera_info_left.disto[4]:.6f}')
        
        # Publish RGB camera info (left camera)
        rgb_info_msg = CameraInfo()
        rgb_info_msg.header.stamp = self.get_clock().now().to_msg()
        rgb_info_msg.header.frame_id = self.camera_frame_id
        
        rgb_info_msg.width = self.camera_info_left.image_size.width
        rgb_info_msg.height = self.camera_info_left.image_size.height
        
        # Intrinsic matrix K (3x3)
        rgb_info_msg.k[0] = self.camera_info_left.fx
        rgb_info_msg.k[1] = 0.0
        rgb_info_msg.k[2] = self.camera_info_left.cx
        rgb_info_msg.k[3] = 0.0
        rgb_info_msg.k[4] = self.camera_info_left.fy
        rgb_info_msg.k[5] = self.camera_info_left.cy
        rgb_info_msg.k[6] = 0.0
        rgb_info_msg.k[7] = 0.0
        rgb_info_msg.k[8] = 1.0
        
        # Rectification matrix R (identity for rectified images)
        rgb_info_msg.r[0] = 1.0
        rgb_info_msg.r[1] = 0.0
        rgb_info_msg.r[2] = 0.0
        rgb_info_msg.r[3] = 0.0
        rgb_info_msg.r[4] = 1.0
        rgb_info_msg.r[5] = 0.0
        rgb_info_msg.r[6] = 0.0
        rgb_info_msg.r[7] = 0.0
        rgb_info_msg.r[8] = 1.0
        
        # Projection matrix P (3x4) - includes baseline for depth processing
        # P = [fx  0  cx  -fx*Tx]
        #     [0  fy  cy  0    ]
        #     [0   0   1   0    ]
        rgb_info_msg.p[0] = self.camera_info_left.fx
        rgb_info_msg.p[1] = 0.0
        rgb_info_msg.p[2] = self.camera_info_left.cx
        rgb_info_msg.p[3] = -self.camera_info_left.fx * self.baseline  # -fx * Tx
        rgb_info_msg.p[4] = 0.0
        rgb_info_msg.p[5] = self.camera_info_left.fy
        rgb_info_msg.p[6] = self.camera_info_left.cy
        rgb_info_msg.p[7] = 0.0
        rgb_info_msg.p[8] = 0.0
        rgb_info_msg.p[9] = 0.0
        rgb_info_msg.p[10] = 1.0
        rgb_info_msg.p[11] = 0.0
        
        # Distortion model
        rgb_info_msg.distortion_model = 'plumb_bob'
        # Assign distortion coefficients safely
        disto = self.camera_info_left.disto
        if len(disto) >= 5:
            rgb_info_msg.d = list(disto[:5])  # Use list assignment for ROS2
        else:
            # If distortion array is shorter, pad with zeros
            rgb_info_msg.d = list(disto) + [0.0] * (5 - len(disto))
        
        self.rgb_camera_info_pub.publish(rgb_info_msg)
        
        # Publish depth camera info (same as RGB for ZED stereo camera)
        depth_info_msg = CameraInfo()
        depth_info_msg.header.stamp = self.get_clock().now().to_msg()
        depth_info_msg.header.frame_id = self.camera_frame_id
        
        depth_info_msg.width = self.camera_info_left.image_size.width
        depth_info_msg.height = self.camera_info_left.image_size.height
        
        # Intrinsic matrix K (3x3)
        depth_info_msg.k[0] = self.camera_info_left.fx
        depth_info_msg.k[1] = 0.0
        depth_info_msg.k[2] = self.camera_info_left.cx
        depth_info_msg.k[3] = 0.0
        depth_info_msg.k[4] = self.camera_info_left.fy
        depth_info_msg.k[5] = self.camera_info_left.cy
        depth_info_msg.k[6] = 0.0
        depth_info_msg.k[7] = 0.0
        depth_info_msg.k[8] = 1.0
        
        # Rectification matrix R (identity for rectified images)
        depth_info_msg.r[0] = 1.0
        depth_info_msg.r[1] = 0.0
        depth_info_msg.r[2] = 0.0
        depth_info_msg.r[3] = 0.0
        depth_info_msg.r[4] = 1.0
        depth_info_msg.r[5] = 0.0
        depth_info_msg.r[6] = 0.0
        depth_info_msg.r[7] = 0.0
        depth_info_msg.r[8] = 1.0
        
        # Projection matrix P (3x4) - includes baseline for depth processing
        depth_info_msg.p[0] = self.camera_info_left.fx
        depth_info_msg.p[1] = 0.0
        depth_info_msg.p[2] = self.camera_info_left.cx
        depth_info_msg.p[3] = -self.camera_info_left.fx * self.baseline  # -fx * Tx
        depth_info_msg.p[4] = 0.0
        depth_info_msg.p[5] = self.camera_info_left.fy
        depth_info_msg.p[6] = self.camera_info_left.cy
        depth_info_msg.p[7] = 0.0
        depth_info_msg.p[8] = 0.0
        depth_info_msg.p[9] = 0.0
        depth_info_msg.p[10] = 1.0
        depth_info_msg.p[11] = 0.0
        
        # Distortion model
        depth_info_msg.distortion_model = 'plumb_bob'
        # Assign distortion coefficients safely
        disto = self.camera_info_left.disto
        if len(disto) >= 5:
            depth_info_msg.d = list(disto[:5])  # Use list assignment for ROS2
        else:
            # If distortion array is shorter, pad with zeros
            depth_info_msg.d = list(disto) + [0.0] * (5 - len(disto))
        
        self.depth_camera_info_pub.publish(depth_info_msg)
    
    def publish_point_cloud(self, point_cloud):
        """Publish point cloud (optional, for visualization)"""
        # An implementation is found in zed_3dpc_node.py
        # Not yet implemented in this class.
        pass
    
    def destroy_node(self):
        """Cleanup on shutdown"""
        if self.zed is not None:
            # Save area memory if enabled
            if self.tracking_enabled and self.area_file_path:
                try:
                    self.zed.save_area_map(self.area_file_path)
                    self.get_logger().info(f"Area memory saved to: {self.area_file_path}")
                except Exception as e:
                    self.get_logger().warn(f"Could not save area memory: {e}")
            
            if self.tracking_enabled:
                self.zed.disable_positional_tracking()
            
            self.zed.close()
        
        super().destroy_node()
